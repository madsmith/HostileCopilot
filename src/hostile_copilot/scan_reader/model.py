from pathlib import Path
from torch import nn
from typing import Any, Callable
import torch
import time


class CRNN(nn.Module):
    def __init__(
        self,
        config: dict[str, Any]
    ):
        super().__init__()

        self.config = self._init_config(config)

        #######################################################################
        # CNN FEATURE EXTRACTOR
        #
        # Goal:
        #   - Reduce the HEIGHT (H) aggressively → make it ~1–2 pixels
        #   - Preserve the WIDTH (W) as much as possible → becomes time steps T
        #   - Expand CHANNELS (C) to create rich feature vectors
        #
        # Starting input shape: (B=Batch Size, 1, H=32, W=variable)
        #
        # Strategy:
        #   - Use 3×3 convolutions with padding=1 → preserve spatial size
        #   - Use MaxPool2d for *downsampling height*, but keep W mostly intact
        #   - Increase channels: 64 → 128 → 256 → 512 as resolution shrinks
        #
        #######################################################################

        # CNN feature extractor
        # This is a neural network that takes an image and outputs a feature
        # sequence for each batch item.  This model learns to detect features
        # such as edges, curves, and shapes in the image.
        self._init_cnn(self.config)

        #######################################################################
        # RESULTING FEATURE MAP SHAPE
        #
        # If input height = 32, the downsampling history is:
        #   32  → 16 → 8 → 4 → 2
        #
        # At the end:
        #   shape = (B, 512, H'=1–2 pixels, W'=~W/2)
        #
        # After squeeze:
        #   shape = (B, 512, T)
        #
        # After permute:
        #   shape = (T, B, 512)
        #######################################################################


        #######################################################################
        # Long Short Term Memory (LSTM) SEQUENCE MODEL
        #
        # Bidirectional LSTM:
        #   - input_size  = 512  (features per timestep)
        #   - hidden_size = 256  (per direction)
        #   - output per timestep = 512 (because 256*2)
        #
        # num_layers=2 = stacking two LSTM layers for slightly deeper context
        #
        #######################################################################
        # This neural network component is a Long Short Term Memory (LSTM) 
        # network that processes the feature sequence generated by the CNN.
        # It is a type of recurrent neural network (RNN) that is designed to
        # process sequential data, such as time series or natural language.
        # This model is bidirectional, meaning it processes the sequence in
        # both forward and backward directions, allowing it to capture
        # dependencies in both directions. It learns to capture the context
        # of the sequence and make predictions based on that context. Where
        # do characters start and end?  Where do they overlap?  Processing
        # distortion... Smoothing..
        self._init_rnn()

        #######################################################################
        # Fully Connected Output
        #
        # Linear layer maps 512 → num_classes (e.g., digits + comma + blank = 12)
        #
        #######################################################################
        # This is a linear layer that maps the output of the RNN to our final
        # prediction.  In this case it connects all .  In this case, the number of classes is the number of
        # possible characters in the alphabet.
        self._init_fc()



    def _init_cnn(self, config: dict[str, Any]):
        # Build CNN layers from config
        def _kwargs(layer: dict[str, Any]) -> dict[str, Any]:
            kwargs = {k: v for k, v in layer.items() if k != "type"}
            return kwargs

        def get_module(layer: dict[str, Any]) -> Callable:
            layer_type = layer["type"]
            if layer_type == "Conv2d":
                return nn.Conv2d
            elif layer_type == "ReLU":
                return nn.ReLU
            elif layer_type == "MaxPool2d":
                return nn.MaxPool2d
            elif layer_type == "BatchNorm2d":
                return nn.BatchNorm2d
            else:
                raise ValueError(f"Unsupported layer type: {layer_type}")
            
        layers = []
        for layer in self.config["cnn_layers"]:
            module = get_module(layer)
            kwargs = _kwargs(layer)
            layers.append(module(**kwargs))

        self.cnn = nn.Sequential(*layers)

        #============================================================
        # Compute CNN output feature size for validation in RNN init
        #============================================================
        input_height = config.get("input_height", 32)
        input_channels = config.get("input_channels", 1)

        # Width doesn't matter for height and channel size, so use a placeholder
        dummy = torch.zeros(1, input_channels, input_height, 200)

        with torch.no_grad():
            out = self.cnn(dummy)

        # Store the static dimensionality
        self._cnn_output_channels = out.shape[1]   # 512
        self._cnn_output_height   = out.shape[2]   # 2
        # The width becomes T = out.shape[3], but it's dynamic

        self._cnn_output_size = (self._cnn_output_channels, self._cnn_output_height)


    def _init_rnn(self):
        """ Initialize the Recurrent Neural Network (RNN). """
        rnn_type      = self.config["rnn_type"]

        if rnn_type == "LSTM":
            rnn_class = nn.LSTM
            rnn_prefix = "lstm_"
        elif rnn_type == "GRU":
            rnn_class = nn.GRU
            rnn_prefix = "gru_"
        else:
            raise ValueError(f"Unknown RNN type: {rnn_type}")

        # gather kwargs
        kwargs = self._config_gather_kwargs(rnn_prefix)

        # validate config
        cnn_channels, cnn_height = self._cnn_output_size
        assert cnn_channels == kwargs["input_size"], \
            f"RNN input_size={kwargs['input_size']} must match CNN output channels={cnn_channels}"
        assert cnn_height in (1, 2), \
            f"CNN output height must be 1–2 pixels for CRNN, got {cnn_height}"
            
        self._rnn_output_size = kwargs["hidden_size"] * (2 if kwargs["bidirectional"] else 1)

        self.rnn = rnn_class(**kwargs)


    def _init_fc(self):
        """ Initialize the fully connected layer. """
        kwargs = self._config_gather_kwargs("fc_")

        # validate config
        assert kwargs["in_features"] == self._rnn_output_size, "FC input size must match RNN output size"
        
        self.fc = nn.Linear(**kwargs)


    def forward(self, x):
        """
        Forward pass through the CRNN model.  Applies the CNN feature extractor,
        then the LSTM, then the final classifier.
        """
        # x: (B - batch item index, 1 - channels, H - height, W - width)
        # Pass through CNN feature extractor model
        conv: nn.Sequential = self.cnn(x)  # (B, C=512, H', W')

        # Keep for debugging
        b, c, h, w = conv.size()
        
        # collapse height dimension (H' becomes 1, ideally)
        # (B - batch item index, C - features, W - width)
        conv = conv.mean(dim=2)  # (B, C, W)
        # We're collapsed to an average feature sequence for each batch item
        # ranging along the image width.

        # convert to (T, B, C), where T = W = sequence length
        # (T - Time steps, B - batch item index, C - features)
        # Time is a bit misleading here but it's a convention when CNNs were
        # for audio signal processing.  In this case, time is the position
        # along the image width.
        conv = conv.permute(2, 0, 1)  # (T, B, C)

        # RNN of shape (T, B, C)
        out, _ = self.rnn(conv)

        # Classify each timestep
        out = self.fc(out)  # (T, B, num_classes)
        return out

    def _init_config(self, override: dict[str, Any] | None = None) -> dict[str, Any]:
        if override is None:
            override = {}

        vocabulary = "0123456789,"  # digits + comma
        if "vocabulary" in override:
            vocabulary = override["vocabulary"]
        num_classes = len(vocabulary) + 1  # + CTC blank

        default_config = {
            # ----------------------
            # Input / Preprocessing
            # ----------------------
            "input_channels": 1,
            "input_height": 32,
            # (Width is variable)

            # ----------------------
            # CNN feature extractor
            # ----------------------
            "cnn_layers": [
                {"type": "Conv2d", "in_channels": 1, "out_channels": 64, "kernel_size": 3, "stride": 1, "padding": 1},
                {"type": "ReLU"},
                {"type": "MaxPool2d", "kernel_size": [2,2]},

                {"type": "Conv2d", "in_channels": 64,  "out_channels": 128, "kernel_size": 3, "stride": 1, "padding": 1},
                {"type": "ReLU"},
                {"type": "MaxPool2d", "kernel_size": [2,2] },

                {"type": "Conv2d", "in_channels": 128, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1},
                {"type": "ReLU"},
                {"type": "Conv2d", "in_channels": 256, "out_channels": 256, "kernel_size": 3, "stride": 1, "padding": 1},
                {"type": "ReLU"},
                {"type": "MaxPool2d", "kernel_size": [2,1]},

                {"type": "Conv2d", "in_channels": 256, "out_channels": 512, "kernel_size": 3, "stride": 1, "padding": 1},
                {"type": "ReLU"},
                {"type": "BatchNorm2d", "num_features": 512},
                {"type": "MaxPool2d", "kernel_size": [2,1]},
            ],

            # ----------------------
            # RNN / Sequence Model
            # ----------------------
            "rnn_type": "LSTM",
            "lstm_input_size": 512,        # must match CNN output
            "lstm_hidden_size": 256,
            "lstm_num_layers": 2,
            "lstm_bidirectional": True,
            "lstm_batch_first": False,

            # ----------------------
            # Fully Connected Output
            # ----------------------
            "fc_in_features": 512,
            "fc_out_features": num_classes,

            # ----------------------
            # Training-time metadata
            # ----------------------
            "vocabulary": vocabulary,
        }

        return self._merge_configs(default_config, override)

    def _config_gather_kwargs(self, prefix: str) -> dict[str, Any]:
        """ Gather kwargs from config with a given prefix """
        return {key[len(prefix):]: value for key, value in self.config.items() if key.startswith(prefix)}
    
    def _merge_configs(self, config: dict[str, Any], override: dict[str, Any]) -> dict[str, Any]:
        """ Merge nested dicts """
        merged = config.copy()
        for key, value in override.items():
            if isinstance(value, dict):
                merged[key] = self._merge_configs(merged[key], value)
            else:
                merged[key] = value
        return merged

class CRNNLoader:
    @classmethod
    def from_weights(cls, weights_path: Path) -> CRNN:
        checkpoint = torch.load(weights_path.as_posix(), map_location="cpu")
        
        config = checkpoint["config"]
        state_dict = checkpoint["state_dict"]

        model = CRNN(config)
        model.load_state_dict(state_dict)
        model.eval()

        return model

    @classmethod
    def save_weights(cls, model: CRNN, weights_path: Path):
        model.config["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%S")

        checkpoint = {
            "state_dict": model.state_dict(),
            "config": model.config,
        }

        torch.save(checkpoint, weights_path.as_posix())

    @classmethod
    def from_script(cls, script_path: Path) -> CRNN:
        scripted = torch.jit.load(script_path.as_posix())
        return scripted

    @classmethod
    def save_script(cls, model: CRNN, weights_path: Path, device: torch.device | None = None):
        if device is None:
            device = "cpu"
            model = model.to(device)
            
        input_height = model.config["input_height"]
        dummy_input = torch.randn(1, 1, input_height, 128).to(device)
        scripted = torch.jit.trace(model, dummy_input)
        scripted.save(weights_path.as_posix())